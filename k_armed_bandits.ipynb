{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Armed Bandits example\n",
    "### Definitions\n",
    "$q_*(a)=E[R_t|A_t=a]$: Value of an arbitrary action $a$ is the expected reward given that $a$ is selected <br/>\n",
    "$a:$ Set of all possible actions <br/>\n",
    "$Q_i(a):$ Action-value estimates at step i for any action <br/>\n",
    "$A_i:$ Action taken at step i <br/>\n",
    "$R_i:$ Reward obtained for action A_i <br/>\n",
    "\n",
    "### Sample averages\n",
    "\n",
    "We need to approximate $q_*(a)$, a simple way to do this is to average the rewards obtained so far for each action, this is called the sample-average method. $Q_t(a)$ converges to $q_*(a)$ for large numbers.  \n",
    "\n",
    "$Q_t(a) = \\frac{\\text{sum of rewards when a taken prior to t}}{\\text{sum of times a taken prior to t}} = \\frac{\\sum_{i=1}^{t-1}R_i \\cdot \\mathrm{I}_{A_i=a}}{\\sum_{i=1}^{t-1}\\mathrm{I}_{A_i=a}}$ \n",
    "\n",
    "Where $\\mathrm{I} = 1$ if $A_i = a$ and $0$ if not, for all $a$. This expression is not computationally efficient since we need to recover all the values prior to $t$, a faster implementation would be to write $Q$ for a single action after it has been selected $n-1$ times as:\n",
    "\n",
    "$Q_n = \\frac{\\sum_{i=1}^{n-1}R_i}{n-1}$\n",
    "\n",
    "$R_i$ is the reward recieved after the $i-th$ selection of the action. Then it is possible to write the incremental formula given $Q_n$ and $R_n$:\n",
    "\n",
    "$Q_{n+1} = \\frac{1}{n}\\sum_{i=1}^{n}R_i =\\frac{1}{n}\\left(R_n + \\frac{n-1}{n-1}\\sum_{i=1}^{n-1}R_i\\right) = \\frac{1}{n}\\left(R_n + (n-1)Q_n\\right) = Q_n + \\frac{1}{n}\\left(R_n-Q_n\\right)$\n",
    "\n",
    "The equation above only needs the information regarding the previous reward and sample average of the last action taken.\n",
    "### Greedy policy\n",
    "Now that we have a way to estimate the values of every action, we need a policy to select which action to take at time step $t$ given the previous values. A greedy approach would be to choose the highest value.\n",
    "\n",
    "$A_t = argmax_a Q_t(a)\\rightarrow$Always exploits, no exploration.\n",
    "#### $\\epsilon$-Greedy\n",
    "Choose action $A_t$ (greedily) with probability $1-\\epsilon$, and choose randomly from all possible actions with probability $\\epsilon$. This allows to exploit most of the time, while having room for exploration.\n",
    "\n",
    "An advantage of this method is that $Q_{t\\rightarrow \\infty}(a) = q_*(a)$ for all $a$, because every action has the possibility ($\\epsilon$) of being chosen, and the probability of choosing the optimal policy is $1-\\epsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1 \n",
    "If $\\epsilon = 0.5$ there is a $1-0.5$ chance of choosing greedily, however, if the selection is random, the greedy option is still available.\n",
    "\n",
    "$P(\\text{greedy}) = P(\\text{greedy}|\\text{greed policy})P(\\text{greed policy}) + P(\\text{greedy}|\\text{random policy})P(\\text{random policy})$\n",
    "\n",
    "$= 1\\cdot (1-\\epsilon) + \\frac{1}{|a|}\\cdot \\epsilon = 0.5+\\frac{0.5}{|a|}$\n",
    "\n",
    "Where $|a|$ is the number of actions available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "actions = np.array([1,2,3,4], dtype = np.float32) # possible actions (\"a\")\n",
    "action_seq = np.array([1,2,2,2,3], np.float32) # Actions taken (A_i)\n",
    "rewards = np.array([1,1,2,2,0], np.float32) #Rewards gained (R_i)\n",
    "Q = np.zeros(actions.shape[0]) #Estimated values\n",
    "\n",
    "def sample_avg(actions, action_seq, rewards):\n",
    "    steps = range(1, len(action_seq)+1)\n",
    "    q = np.zeros((len(steps)+1, len(actions)))\n",
    "    for t in steps:\n",
    "        num = np.array([rewards[:t][action_seq[0:t]==i] for i in actions])\n",
    "        num = np.array([np.sum(num[i]) for i in range(len(num))])\n",
    "        den = np.sum(np.array([action_seq[0:t]==i for i in actions]), axis = 1)\n",
    "        q[t,:]=np.where(den == 0, den, num/den)\n",
    "    return q\n",
    "q = sample_avg(actions, action_seq, rewards)\n",
    "print(q, action_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Greedy (breaking ties randomly) or $\\epsilon$, all actions have same Q.<br/>\n",
    "Step 2: $\\epsilon$, didn't choose highest Q action.<br/>\n",
    "Step 3: Greedy (breaking ties randomly), options 1 and 2 have the same rewards.<br/>\n",
    "Step 4: Greedy or $\\epsilon$, option with highest Q is chosen, could be for greed or randomness.  \n",
    "Step 5: $\\epsilon$, didn't choose highest Q action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\epsilon$-greedy method implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class EpsilonGreedy():\n",
    "    def __init__(self, n_actions = 10, epsilon = 0, steps = 3000, \n",
    "                 iterations = 200, mean = 0, std = 1, q_real = None):\n",
    "        #Initialize parameters\n",
    "        self.n_actions = n_actions\n",
    "        self.epsilon = epsilon\n",
    "        self.steps = steps #how many steps make 1 run\n",
    "        self.iterations = iterations #how many runs to make\n",
    "        self.stats = {'mean': mean, 'std': std}\n",
    "\n",
    "        #Generate true expected values q*\n",
    "        q_gen =  tfp.distributions.Normal(loc=0, scale=1)\n",
    "        if q_real is None:\n",
    "            self.q_real = q_gen.sample([self.n_actions]).numpy()\n",
    "        else:\n",
    "            self.q_real = q_real\n",
    "            \n",
    "        #Normal dist. for sampling rewards with mean = q* and std = 1\n",
    "        self.q_real_dist = []\n",
    "        for i in self.q_real:\n",
    "            self.q_real_dist.append(tfp.distributions.Normal(loc=i, scale=self.stats['std']))\n",
    "        \n",
    "        self.A_seq = np.empty((self.steps, self.iterations))\n",
    "        self.R_seq = np.empty((self.steps, self.iterations))\n",
    "    \n",
    "    def plot_true_values(self, samples):\n",
    "        q_real_dist = tfp.distributions.Normal(loc=self.q_real, scale = self.stats['std'])\n",
    "        R = q_real_dist.sample(samples).numpy().T.reshape(-1)\n",
    "        s = np.repeat([i for i in range(1, self.n_actions+1)], samples)\n",
    "        df = pd.DataFrame(data = [s, R]).T\n",
    "        df.columns = ['Action','Reward distributions']\n",
    "        plt.figure()\n",
    "        sns.violinplot(x = 'Action', y = 'Reward distributions', data = df)\n",
    "        \n",
    "    def sample_average(self, timer = True, freq = 200):\n",
    "        start = time.time()\n",
    "        iter_start = time.time()\n",
    "        for i in range(self.iterations):\n",
    "            Q = [0]*self.n_actions #Estimated value\n",
    "            N = Q.copy() #Number of ocurrences\n",
    "            for t in range(self.steps):\n",
    "                if 1 - self.epsilon > random.random(): #exploitation\n",
    "                    Q_argmax = [i for i, j in enumerate(Q) if j == max(Q)]\n",
    "                    if len(Q_argmax) == 1:\n",
    "                        A = Q_argmax[0]\n",
    "                    else: #break ties randomly\n",
    "                        A = random.choice(Q_argmax)\n",
    "                else: #exploration\n",
    "                    A = np.random.randint(self.n_actions)\n",
    "                R = self.q_real_dist[A].sample().numpy()\n",
    "                N[A] = N[A] + 1\n",
    "                self.A_seq[t, i] = A\n",
    "                self.R_seq[t, i] = R\n",
    "                Q[A] = Q[A] + 1/N[A]*(R-Q[A])               \n",
    "            if timer and i%freq == 0:\n",
    "                print(\"Run {}: {:.2f} seconds\".format(i+1, time.time() - iter_start))\n",
    "                iter_start = time.time()\n",
    "        print(\"Elapsed time: {:.2f} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_1 = EpsilonGreedy(epsilon = 0)\n",
    "test_2 = EpsilonGreedy(epsilon = .01, q_real = test_1.q_real) #real values are the same for all tests\n",
    "test_3 = EpsilonGreedy(epsilon = .1, q_real = test_1.q_real)\n",
    "\n",
    "test_1.plot_true_values(1000)\n",
    "test_2.plot_true_values(1000)\n",
    "test_3.plot_true_values(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_time = True\n",
    "test_1.sample_average(timer = show_time)\n",
    "test_2.sample_average(timer = show_time)\n",
    "test_3.sample_average(timer = show_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rwd_1 = np.mean(test_1.R_seq, axis = 1)\n",
    "rwd_2 = np.mean(test_2.R_seq, axis = 1)\n",
    "rwd_3 = np.mean(test_3.R_seq, axis = 1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(rwd_1, 'g', label = r'$\\epsilon = 0$')\n",
    "plt.plot(rwd_2, 'r', label = r'$\\epsilon = 0.01$')\n",
    "plt.plot(rwd_3, 'k', label = r'$\\epsilon = 0.1$')\n",
    "plt.legend()\n",
    "\n",
    "err_1 = np.mean(test_1.A_seq == np.argmax(test_1.q_real), axis = 1)\n",
    "err_2 = np.mean(test_2.A_seq == np.argmax(test_2.q_real), axis = 1)\n",
    "err_3 = np.mean(test_3.A_seq == np.argmax(test_3.q_real), axis = 1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(err_1, 'g', label = r'$\\epsilon = 0$')\n",
    "plt.plot(err_2, 'r', label = r'$\\epsilon = 0.01$')\n",
    "plt.plot(err_3, 'k', label = r'$\\epsilon = 0.1$')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3\n",
    "$\\epsilon = .01$ will perform better in the long run, since eventually will explore every action and select the best one $99\\%$ of the time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
