{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Armed Bandits example\n",
    "### Definitions\n",
    "$q_*(a)=E[R_t|A_t=a]$: Value of an arbitrary action $a$ is the expected reward given that $a$ is selected <br/>\n",
    "$a:$ Set of all possible actions <br/>\n",
    "$Q_i(a):$ Action-value estimates at step i for any action <br/>\n",
    "$A_i:$ Action taken at step i <br/>\n",
    "$R_i:$ Reward obtained for action A_i <br/>\n",
    "\n",
    "### Sample average\n",
    "$Q_t(a) = \\frac{\\text{sum of rewards when a taken prior to t}}{\\text{sum of times a taken prior to t}} = \\frac{\\sum_{i=1}^{t-1}R_i \\cdot \\mathrm{I}_{A_i=a}}{\\sum_{i=1}^{t-1}\\mathrm{I}_{A_i=a}}$ \n",
    "\n",
    "Where $\\mathrm{I} = 1$ if $A_i = a$ and $0$ if not.\n",
    "\n",
    "### Greedy policy\n",
    "$A_t = argmax_a Q_t(a)\\rightarrow$Always exploits, no exploration.\n",
    "#### $\\epsilon$-Greedy\n",
    "Choose action $A_t$ (greedily) with probability $1-\\epsilon$, and choose randomly from all possible actions with probability $\\epsilon$. This allows to exploit most of the time, while having room for exploration.\n",
    "\n",
    "An advantage of this method is that $Q_{t\\rightarrow \\infty}(a) = q_*(a)$, and the probability of choosing the optimal policy is $1-\\epsilon$\n",
    "\n",
    "## Calculating Q_t(a) with sample average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.        ]\n",
      " [1.         0.         0.         0.        ]\n",
      " [1.         1.         0.         0.        ]\n",
      " [1.         1.5        0.         0.        ]\n",
      " [1.         1.66666667 0.         0.        ]\n",
      " [1.         1.66666667 0.         0.        ]] [1. 2. 2. 2. 3.]\n"
     ]
    }
   ],
   "source": [
    "actions = np.array([1,2,3,4], dtype = np.float32) # possible actions (\"a\")\n",
    "action_seq = np.array([1,2,2,2,3], np.float32) # Actions taken (A_i)\n",
    "rewards = np.array([1,1,2,2,0], np.float32) #Rewards gained (R_i)\n",
    "Q = np.zeros(actions.shape[0]) #Estimated values\n",
    "\n",
    "def sample_avg(actions, action_seq, rewards):\n",
    "    steps = range(1, len(action_seq)+1)\n",
    "    q = np.zeros((len(steps)+1, len(actions)))\n",
    "    for t in steps:\n",
    "        num = np.array([rewards[:t][action_seq[0:t]==i] for i in actions])\n",
    "        num = np.array([np.sum(num[i]) for i in range(len(num))])\n",
    "        den = np.sum(np.array([action_seq[0:t]==i for i in actions]), axis = 1)\n",
    "        q[t,:]=np.where(den == 0, den, num/den)\n",
    "    return q\n",
    "q = sample_avg(actions, action_seq, rewards)\n",
    "print(q, action_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Random, all actions have same Q.<br/>\n",
    "Step 2: $\\epsilon$, didn't choose highest Q action.<br/>\n",
    "Step 3: Random, options 1 and 2 have the same rewards.<br/>\n",
    "Step 4: Greedy or $\\epsilon$, option with highest Q is chosen, could be for greed or randomness.  \n",
    "Step 5: $\\epsilon$, didn't choose highest Q action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-e1164e596735>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Elapsed time: {:.2f} seconds\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0mtest_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEpsilonGreedy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0mtest_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEpsilonGreedy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_real\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0mtest_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEpsilonGreedy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_real\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-e1164e596735>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, n_actions, epsilon, steps, iterations, mean, std, q_real)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m#Normal with mean = q*, std = 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_real_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_real\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_real_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'std'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "class EpsilonGreedy():\n",
    "    def __init__(self, n_actions = 10, epsilon = 0, steps = 1000, \n",
    "                 iterations = 2000, mean = 0, std = 1, q_real = None):\n",
    "        #Initialize parameters\n",
    "        self.n_actions = n_actions\n",
    "        self.epsilon = epsilon\n",
    "        self.steps = steps\n",
    "        self.iterations = iterations\n",
    "        self.stats = {'mean': mean, 'std': std}\n",
    "\n",
    "        #Generate true expected values q*\n",
    "        q_gen =  tfp.distributions.Normal(loc=0, scale=1)\n",
    "        if q_real is not None:\n",
    "            self.q_real = q_gen.sample([self.n_actions]).numpy()\n",
    "        else:\n",
    "            self.q_real = q_real\n",
    "        \n",
    "        #Normal with mean = q*, std = 1\n",
    "        self.q_real_dist = []\n",
    "        for i in self.q_real:\n",
    "            self.q_real_dist.append(tfp.distributions.Normal(loc=i, scale=self.stats['std']))\n",
    "        \n",
    "        \n",
    "        self.A_seq = np.empty((self.steps, self.iterations))\n",
    "        self.R_seq = np.empty((self.steps, self.iterations))\n",
    "        #self.N_seq = np.empty((self.steps, self.n_actions))\n",
    "        #self.Q_seq = np.empty((self.steps, self.n_actions))\n",
    "    \n",
    "    def plot_true_values(self, samples):\n",
    "        q_real_dist = tfp.distributions.Normal(loc=self.q_real, scale = self.stats['std'])\n",
    "        R = q_real_dist.sample(samples).numpy().T.reshape(-1)\n",
    "        s = np.repeat([i for i in range(1, self.n_actions+1)], samples)\n",
    "        df = pd.DataFrame(data = [s, R]).T\n",
    "        df.columns = ['Action','Reward distributions']\n",
    "        plt.figure()\n",
    "        sns.violinplot(x = 'Action', y = 'Reward distributions', data = df)\n",
    "        \n",
    "    def sample_average(self):\n",
    "        start = time.time()\n",
    "        iter_start = time.time()\n",
    "        for i in range(self.iterations):\n",
    "            Q = [0]*self.n_actions\n",
    "            N = Q.copy()\n",
    "            for t in range(self.steps):\n",
    "                if 1 - self.epsilon > random.random(): #exploitation\n",
    "                    Q_argmax = [i for i, j in enumerate(Q) if j == max(Q)] #break ties randomly  \n",
    "                    if len(Q_argmax) == 1:\n",
    "                        A = Q_argmax[0]\n",
    "                    else: #break ties randomly\n",
    "                        A = random.choice(Q_argmax)\n",
    "                else: #exploration\n",
    "                    A = np.random.randint(self.n_actions)\n",
    "\n",
    "                R = self.q_real_dist[A].sample().numpy()\n",
    "                N[A] = N[A] + 1\n",
    "                self.A_seq[t, i] = A\n",
    "                self.R_seq[t, i] = R\n",
    "                #self.N_seq[t] = N\n",
    "                #self.Q_seq[t] = Q\n",
    "\n",
    "                Q[A] = Q[A] + 1/N[A]*(R-Q[A])\n",
    "                \n",
    "            if i%200 == 0:\n",
    "                print(\"Run {}: {:.2f} seconds\".format(i+1, time.time() - iter_start))\n",
    "                iter_start = time.time()\n",
    "        print(\"Elapsed time: {:.2f} seconds\".format(time.time() - start))\n",
    "\n",
    "test_1 = EpsilonGreedy(epsilon = 0)\n",
    "test_2 = EpsilonGreedy(epsilon = .01, q_real = test_1.q_real)\n",
    "test_3 = EpsilonGreedy(epsilon = .1, q_real = test_1.q_real)\n",
    "\n",
    "test_1.plot_true_values(1000)\n",
    "test_2.plot_true_values(1000)\n",
    "test_3.plot_true_values(1000)\n",
    "\n",
    "#test_1.sample_average()\n",
    "#test_2.sample_average()\n",
    "#test_3.sample_average()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rwd_1 = np.mean(test_1.R_seq, axis = 1)\n",
    "rwd_2 = np.mean(test_2.R_seq, axis = 1)\n",
    "rwd_3 = np.mean(test_3.R_seq, axis = 1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(rwd_1, 'g', label = r'$\\epsilon = 0$')\n",
    "plt.plot(rwd_2, 'r', label = r'$\\epsilon = 0.01$')\n",
    "plt.plot(rwd_3, 'k', label = r'$\\epsilon = 0.1$')\n",
    "plt.legend()\n",
    "\n",
    "err_1 = np.mean(test_1.A_seq == np.argmax(test_1.q_real), axis = 1)\n",
    "err_2 = np.mean(test_2.A_seq == np.argmax(test_2.q_real), axis = 1)\n",
    "err_3 = np.mean(test_3.A_seq == np.argmax(test_3.q_real), axis = 1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(err_1, 'g', label = r'$\\epsilon = 0$')\n",
    "plt.plot(err_2, 'r', label = r'$\\epsilon = 0.01$')\n",
    "plt.plot(err_3, 'k', label = r'$\\epsilon = 0.1$')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "if None:\n",
    "    print('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
